{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- !pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE WE ARE TRAINING THE ADAPTERS NOT THE original  WEIGHTS\n",
    "# We will be adding weights to the models are various points and fine-tuning those weights \n",
    "# We will set up the huggingface hub at the start so that we can later save our weights to HF hub\n",
    "# the token is a write token which is saved by the name - PEFT-LORA-CHECKPOINTS\n",
    "x\n",
    "from  huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# bitsandbytes from HF turns your model into 8 bits\n",
    "# Model won't take up a lot of GPU ram \n",
    "# Makes it easier and faster to store \n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer , AutoConfig , AutoModelForCausalLM\n",
    "model  = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloomz-560m\" , \n",
    "    load_in_8bit = True , \n",
    "    device_map =\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
    "\n",
    "# WARNINGS: The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions.\n",
    "#  Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original colab notebook will be posted after the code is complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  # freeze the model - train adapters later\n",
    "  param.requires_grad = False\n",
    "  if param.ndim==1:\n",
    "    # cast the small params[dim=1] (eg.layernorm) to float32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "\n",
    "#reduce number of stored activations\n",
    "model.gradient_checkpointing_enable()\n",
    "# enabling input gradients \n",
    "model.enable_input_require_grads()\n",
    "\n",
    "\n",
    "# cast the numerical output to float32 for stability , wrapper around sequential layers \n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self,x):\n",
    "    return super().forward(x).to(torch.float32)\n",
    "\n",
    "# lm_head is the model's outout layer , wrap it with castoutputtofloat func, \n",
    "#  Ensures that the logits (outputs of the lm_head) are cast to float32, making them\n",
    "#  more stable for tasks like computing loss or metrics during training.\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "\n",
    "# In PyTorch, the requires_grad attribute of a tensor indicates whether or not \n",
    "# PyTorch should compute gradients for that tensor during the backward pass.\n",
    "# requires_grad=True: Enables gradient computation for the tensor.\n",
    "# When requires_grad=False, PyTorch does not track operations, so no gradients are computed, \n",
    "# and the tensor is treated as a constant.\n",
    "#  It is used To specify which parameters in a model should be updated during training.\n",
    "# To save memory and computation time by disabling gradient tracking for parameters that don't need to be updated.\n",
    "\n",
    "# 1D params are 1D tensors\n",
    "# ndim==1 checks if the parameter is a 1D tensor (like biases or weights in layer normalization) and casts it to float32.\n",
    "# In mixed precision training (e.g., using float16 for efficiency), small parameters like biases or layer norm weights can cause numerical instability.\n",
    "#  Converting these parameters to float32 helps ensure stability without significantly affecting performance.\n",
    "\n",
    "# Reducing Stored Activations : Enables gradient checkpointing, which reduces the memory usage for activations during training by \n",
    "# recomputing them during backpropagation instead of storing them.\n",
    "# This trade-off between computation and memory usage is particularly useful for training large models with limited GPU memory.\n",
    "\n",
    "\n",
    "#  Enabling Input Gradients: This ensures that gradients with respect to the modelâ€™s inputs are computed.\n",
    "# Necessary when applying techniques like LoRA (Low-Rank Adaptation) or other methods that modify intermediate layers during fine-tuning.\n",
    "#  This enables the model to propagate gradients correctly for such custom layers or inputs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
