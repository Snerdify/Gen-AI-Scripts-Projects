{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- !pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE WE ARE TRAINING THE ADAPTERS NOT THE original  WEIGHTS\n",
    "# We will be adding weights to the models are various points and fine-tuning those weights \n",
    "# We will set up the huggingface hub at the start so that we can later save our weights to HF hub\n",
    "# the token is a write token which is saved by the name - PEFT-LORA-CHECKPOINTS\n",
    "x\n",
    "from  huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# bitsandbytes from HF turns your model into 8 bits\n",
    "# Model won't take up a lot of GPU ram \n",
    "# Makes it easier and faster to store \n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer , AutoConfig , AutoModelForCausalLM\n",
    "model  = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloomz-560m\" , \n",
    "    load_in_8bit = True , \n",
    "    device_map =\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
    "\n",
    "# WARNINGS: The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions.\n",
    "#  Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original colab notebook will be posted after the code is complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  # freeze the model - train adapters later\n",
    "  param.requires_grad = False\n",
    "  if param.ndim==1:\n",
    "    # cast the small params[dim=1] (eg.layernorm) to float32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "\n",
    "#reduce number of stored activations\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self,x):\n",
    "    return super().forward(x).to(torch.float32)\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
