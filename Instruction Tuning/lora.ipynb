{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- !pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE WE ARE TRAINING THE ADAPTERS NOT THE original  WEIGHTS\n",
    "# We will be adding weights to the models are various points and fine-tuning those weights \n",
    "# We will set up the huggingface hub at the start so that we can later save our weights to HF hub\n",
    "# the token is a write token which is saved by the name - PEFT-LORA-CHECKPOINTS\n",
    "x\n",
    "from  huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# bitsandbytes from HF turns your model into 8 bits\n",
    "# Model won't take up a lot of GPU ram \n",
    "# Makes it easier and faster to store \n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer , AutoConfig , AutoModelForCausalLM\n",
    "model  = AutoModelForCausalLM.from_pretrained(\n",
    "    \"bigscience/bloomz-560m\" , \n",
    "    load_in_8bit = True , \n",
    "    device_map =\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
    "\n",
    "# WARNINGS: The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions.\n",
    "#  Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original colab notebook will be posted after the code is complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  # freeze the model - train adapters later\n",
    "  param.requires_grad = False\n",
    "  if param.ndim==1:\n",
    "    # cast the small params[dim=1] (eg.layernorm) to float32 for stability\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "\n",
    "#reduce number of stored activations\n",
    "model.gradient_checkpointing_enable()\n",
    "# enabling input gradients \n",
    "model.enable_input_require_grads()\n",
    "\n",
    "\n",
    "# cast the numerical output to float32 for stability , wrapper around sequential layers \n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "  def forward(self,x):\n",
    "    return super().forward(x).to(torch.float32)\n",
    "\n",
    "# lm_head is the model's outout layer , wrap it with castoutputtofloat func, \n",
    "#  Ensures that the logits (outputs of the lm_head) are cast to float32, making them\n",
    "#  more stable for tasks like computing loss or metrics during training.\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "\n",
    "\n",
    "# In PyTorch, the requires_grad attribute of a tensor indicates whether or not \n",
    "# PyTorch should compute gradients for that tensor during the backward pass.\n",
    "# requires_grad=True: Enables gradient computation for the tensor.\n",
    "# When requires_grad=False, PyTorch does not track operations, so no gradients are computed, \n",
    "# and the tensor is treated as a constant.\n",
    "#  It is used To specify which parameters in a model should be updated during training.\n",
    "# To save memory and computation time by disabling gradient tracking for parameters that don't need to be updated.\n",
    "\n",
    "# 1D params are 1D tensors\n",
    "# ndim==1 checks if the parameter is a 1D tensor (like biases or weights in layer normalization) and casts it to float32.\n",
    "# In mixed precision training (e.g., using float16 for efficiency), small parameters like biases or layer norm weights can cause numerical instability.\n",
    "#  Converting these parameters to float32 helps ensure stability without significantly affecting performance.\n",
    "\n",
    "# Reducing Stored Activations : Enables gradient checkpointing, which reduces the memory usage for activations during training by \n",
    "# recomputing them during backpropagation instead of storing them.\n",
    "# This trade-off between computation and memory usage is particularly useful for training large models with limited GPU memory.\n",
    "\n",
    "\n",
    "#  Enabling Input Gradients: This ensures that gradients with respect to the modelâ€™s inputs are computed.\n",
    "# Necessary when applying techniques like LoRA (Low-Rank Adaptation) or other methods that modify intermediate layers during fine-tuning.\n",
    "#  This enables the model to propagate gradients correctly for such custom layers or inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETTING UP LORA ADAPTERS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of the function is to calculate and print the number of trainable parameters versus the total number of parameters in the model.\n",
    "# _: The name of the parameter is ignored in this loop since it is not needed.\n",
    "# param: Represents the actual parameter tensor.\n",
    "# numel stands for number of elements \n",
    "# For a tensor with a specific shape, .numel() computes the product of its dimensions to determine how many elements the tensor contains.\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "  trainable_params=0\n",
    "  all_params = 0\n",
    "  for _,param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "  # if we are setting requires_grad = True , that means we are computing backpropogation for those params, hence they will be included in training \n",
    "\n",
    "      trainable_params += param.numel()\n",
    "    print(f\"trainable params:{trainable_params} || all_params:{all_params} || trainable% :{100*trainable_params/all_params}\")\n",
    "\n",
    "\n",
    "'''\n",
    "Why Use .numel()? : Returns total number of params in a tensor\n",
    "Count Parameters: When iterating over model parameters, .numel() helps determine the total number of elements in each parameter tensor.\n",
    "Memory Estimation: Knowing the number of elements allows you to estimate the memory required for storing the tensor.\n",
    "Debugging: Helps ensure the expected number of elements in a tensor, especially when dealing with reshaping or slicing operations.\n",
    "\n",
    "\n",
    "LoRA: Fine-tunes only a small subset of parameters (low-rank adapters) while freezing most of the original model parameters.\n",
    "This function helps verify that the model is correctly set up for LoRA by ensuring only the intended parameters are trainable.\n",
    "During LoRA setup, you'd expect the trainable% to be very small (e.g., <1%), as most parameters remain frozen.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig , get_peft_model\n",
    "\n",
    "config  = LoraConfig(\n",
    "    r =16 ,# attention heads \n",
    "    lora_alpha = 32 ,   #alpha scaling\n",
    "    # target_modules = [\"q_proj\",\"v_proj\"]\n",
    "    lora_dropout = 0.05 , \n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\" # set this for CLM or Seq2seq\n",
    ")\n",
    "model = get_peft_model(model,config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# LoraConfig: Defines the configuration for LoRA fine-tuning, specifying the settings for how the low-rank adaptation is applied\n",
    "# get_peft_model: A utility function to apply LoRA to an existing model, integrating the LoRA parameters into the model's architecture.\n",
    "'''\n",
    "r=16\n",
    "Specifies the rank of the low-rank matrices used in LoRA.\n",
    "In LoRA, instead of training the full parameter matrices of the model, a pair of low-rank matrices (rank r) is trained and added to the original weights.\n",
    "Higher r values allow the model to learn more complex adaptations but require more compute and memory.\n",
    "\n",
    "lora_alpha=32\n",
    "A scaling factor applied to the LoRA updates.\n",
    "Controls how much influence the LoRA layers have on the overall model's behavior.\n",
    "Larger values of lora_alpha scale the LoRA updates more aggressively.\n",
    "\n",
    "target_modules=[\"q_proj\", \"v_proj\"] \n",
    "Specifies which modules in the model are targeted for LoRA adaptation.\n",
    "For transformer-based models:\n",
    "q_proj refers to the query projection in the attention mechanism.\n",
    "v_proj refers to the value projection.\n",
    "By focusing only on specific modules, LoRA reduces the number of trainable parameters and computation cost.\n",
    "\n",
    "lora_dropout=0.05\n",
    "Introduces a small dropout rate to the LoRA updates.\n",
    "Helps improve generalization by randomly dropping a portion of the LoRA updates during training.\n",
    "\n",
    "bias=\"none\"\n",
    "Specifies how the bias terms in the model are handled.\n",
    "\"none\": No bias terms are added or trained.\n",
    "\"all\": Bias terms are trainable.\n",
    "\"lora_only\": Only biases corresponding to LoRA modules are trainable.\n",
    "\n",
    "\n",
    "task_type=\"CAUSAL_LM\"\n",
    "Defines the type of task the model is being fine-tuned for.\n",
    "\"CAUSAL_LM\": Indicates the task is causal language modeling (e.g., autoregressive text generation).\n",
    "Other possible task types include \"SEQ2SEQ_LM\" (sequence-to-sequence tasks) or \"TOKEN_CLASSIFICATION\".\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "# data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Give a quote by the user , predict its tag\n",
    "def merge_columns(example):\n",
    "  # tell the model : every time it sees (->)\n",
    "  # condition on the input before that , and generate the tags after that \n",
    "  # \n",
    "  example[\"prediction\"] = example[\"quote\"] + \"->:\"+str(example[\"tags\"])\n",
    "  return example\n",
    "\n",
    "\n",
    "data[\"train\"] = data[\"train\"].map(merge_columns)\n",
    "data[\"train\"][\"prediction\"][3:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda samples:tokenizer(samples[\"prediction\"]),batched= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "# extra columns : prediction , input_ids , attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model = model ,\n",
    "    train_dataset = data[\"train\"],\n",
    "    args = transformers.TrainingArguments(\n",
    "# training on 1 gpu \n",
    "        per_device_train_batch_size =4,\n",
    "        gradient_accumulation_steps = 4 , \n",
    "    # we set a warmup to start with an extremely low learning rate and\n",
    "    # build it up from there to 2e-4\n",
    "        warmup_steps = 100,\n",
    "        max_steps = 200 ,\n",
    "        learning_rate = 2e-4 ,\n",
    "        fp16 = True,\n",
    "        logging_steps = 1,\n",
    "        output_dir =\"outputs\"\n",
    "    ) , \n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer,mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Save the adapters on the HUB\n",
    "model.push_to_hub(\"goffer/bloomz-560m-lora\",\n",
    "                   use_auth_token= True,\n",
    "                   commit_message= \"LORA Adapters\",\n",
    "                   private = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
